\documentclass[10pt,a4paper]{article}

\usepackage{balance}
\usepackage{lrec2006}

\title{Environmental Audio Immersion from Fictional Narratives}
\name{Leon Derczynski, Nanna Inie}
\address{University of Sheffield \\ S1 4DP, UK \\ {\tt leon@dcs.shef.ac.uk} \\ \\
		 Aarhus University \\ Aabogade 34, 8200, Denmark \\ {\tt inie@cavi.au.dk}}

\abstract{Experiencing a narrative can be made more enjoyable and powerful by increasing the level of the immersion in the story.
This can be achieved through contextual cues, such as sounds and images.
In order to provide these cues automatically, we attempt to detect parts of the environment described in a fictional narrative.
We present machine learning approaches to this problem, compare them against to a rule-engineered baseline, and evaluate them over a dataset of fictional texts.
In addition, we analyse the performance of this system over the output of a text-to-speech system taking the same narratives as input.
This audio dataset is made available.
The system is able to recognise environmental context and respond appropriately.}

\begin{document}

\maketitleabstract

\section{Introduction}

Non-verbal cues can improve the experience of a narrative. 
For example, adding sound qualitatively impacts the recipient's mood, and is useful for increasing immersion and positive narrative experiences~\cite{ermi2005fundamental,madden2009collaborative,huiberts2010captivating}.
An immersive environment is most engaging when participants believe their actions affect the environment; reliable and responsive cues such as sounds are important for this~\cite{bobick1999kidsroom}.
However, providing such cues can be an expensive manual process, and many textual narratives do not have accompanying contextual media such as background sounds or images.
An automatic approach could be taken to building such context.
The research question we address is: how can we automatically identify in narratives environments and events that can be associated with external cues?

Taking narratives as input, we attempt to determine what the appropriate environmental component of the context is, which can then be used to provide cues (such as ambient sounds).
There is a diverse range of potential environments from which cues can be generated, even in a constrained corpus of narratives.
In addition, effects (e.g. video or audio cues) are required for surprise events described in a story, like an approaching horse or a thunderstorm.
This is similar to streamed topic extraction~\cite{allan2002introduction,preotiuc2012trendminer}.
We determine a corpus, frame the task by giving a specific set of environments for which cues are available, and then evaluate system performance over this dataset.
Finally, we create a new language resource of audio recordings and automatic transcriptions, used to evaluate the system on input closer to that it might receive when operating in real-time.


\section{Background}

- NANNA CLEVER WORDS HERE - 

What's ``immersion"?

Why should we care?

What's important when creating environmental cues for improved narrative immersion?

\section{Method}

In general, we cast the problem as one of document classification.
Some special characteristics are included, namely that documents can have more than one class; and that documents may have not belong to any class at all, i.e., not every text requires or activates an environmental cue.

\begin{figure}
\centering
\label{fig:ff-para}
\caption{Example narrative text}
\end{figure}

\subsection{Dataset}
Our ideal requirements are that a dataset be structures as instances, each being a focused passage of text, with most containing descriptions of environments or events.
For this dataset, we used paragraphs from a set of role-playing books.%~\cite{fangs,crypt,poe,sswamp}.
%HIDDEN FOR REVIEW
An example text is given in Figure~\ref{fig:ff-para}
Each paragraph is set in a distinct location, each possibly combined with the description of an event, making manual environment annotation simple and distinct.
In comparison, the bounds of passages of text relevant to a particular cue are harder to detemine in running narrative text from e.g. a fictional novel .
The set of potential environmental items is:

\begin{itemize}
\item {\bf Environments:} Mountain; Hill; Forest; Swamp; Meadow; Road; Town; Crowd; Tavern; Underground.
\item {\bf Ambience:} Windy; Blizzard; Rain; Lightning; Stream; River; Campfire; Night.
\item {\bf Events:} Trotting horse; Galloping horse; Many galloping horses; Thunder.
\end{itemize}

Paragraphs were labelled with one or more of these labels by a human annotator.
In total, 203 paragraphs were labelled, with 13884 words.

\subsection{Spoken dataset}
An eventual use of this system is to provide automatic sound effects in real-time for a story that is read out loud by a human narrator.
Accordingly, we attempt to train and evaluate the system based on the output of a speech recognition system.
This requires an audio dataset and the automatic transcription of audio content, and the subsequent analysis of the text output.

Paragraphs were read by an English native speaker, and recorded.
A speech recognition system~\cite{lamere2003design} then interpreted these readings and generated a textual version of each paragraph.
The labels used in the text input corpus were then associated with these outputs.
This constitutes the transcribed dataset.

\subsection{Baseline}
As a baseline, we use a gazetteer trigger words that match the name of the cue.
If a word occurs in a candidate passage, then that cue is triggered.
For example, if there is a cue named ``swamp", the passage ``He marches through the swamp, sweating from his brow" will trigger that cue using the baseline system.

\subsection{Features and Classification}

It is better to provide environmental responses as soon as possible after input.
Therefore, we break paragraphs into sentences and label at sentence level, allowing a response to be given after processing a sentence instead of waiting for the paragraph to end.
To build sentence-level representations, we use the sentence2vec tool to map candidate sentences into a $k-$dimensional continuous embedded space.
This relies on the word2vec tool, which takes large tracts of unlabeled text as input and builds vector representations of words based on their distributional properties.

The genre used to generate these had a significant effect on result quality.
For example, using word2vec embeddings generated from two billion Google News articles, the top-ranking non-punctuation match for the sentence \emph{``The going underfoot becomes muddier, until eventually you reach an area where bulrushes tower over your head."} was the phrase {\em Maria\_Sharapova} -- not an intuitively relevant result.
News is intrinsically constrained in topic and style, and out-of-domain for fictional narratives.

Therefore, we used embeddings learned from the multi-topic Brown corpus~\cite{francis1979brown}, which includes large amounts of narrative and fictional text.
The sections used were F (Popular Lore), K (Fiction: General), and N (Fiction: Adventure and Western).
In addition, we used a digital copy of {\em The Lord of the Rings}.
This yield a sample of roughly 820K words over which distributional embeddings were induced.

We frame this as a multi-label, multi-class learning problem.
That is, there are many potential classes in terms of sounds to be played, and also, more than one of these may apply to a given input.
We take a one-vs-all approach to classification, training a binary classifier for each label, and apply all classifiers to a sentence as it comes in.

We note that false positives have a higher penalty than false negatives.
It is disruptive to receive the wrong cue.
For example, hearing battle noises when one should hear a babbling brook (false positive) is detrimental, and worse than hearing nothing (false negative).
In order to bias classifications in this direction, we experiment with the SVM Cost parameter~\cite{morik1999combining}.

Further, we represent this immersion disruption in the evalation measure.
Given precision $P$ and recall $R$, typically an F-score is drawn from $F_\beta$ with $\beta=1$.

\begin{equation}
F_\beta = (1+\beta^2)\frac{PR}{(\beta^2 P) + R} 
\end{equation}


Firstly, we try a bag-of-words representation using a multinomial Bayes classifier.
In addition, we represent paragraphs as average vectors in n-dimensional space by taking the cosine product of embeddings of words in the paragraph, and then learn a binary SVM for each cue based on these representations.





\begin{table*}
\centering
\footnotesize
\begin{tabular}{c|cccc}
\hline
{\bf Approach} & {\bf Precision} & {\bf Recall} & {\bf $F_1$ score} & {\bf $F_2$ score} \\
\hline
Baseline: trigger words &&&& \\
BoW + Multinomial Bayes &&&& \\
s2v + SVM &&&& \\
\hline
\end{tabular}
\label{tab:results-main}
\caption{Classification accuracy}
\end{table*}

\begin{table*}
\centering
\footnotesize
\begin{tabular}{c|cccc}
\hline
{\bf Cost $C$} & {\bf Precision} & {\bf Recall} & {\bf $F_1$ score} & {\bf $F_2$ score} \\
\hline
0.1 &&&& \\
0.2 &&&& \\
0.5 &&&& \\
0.8 &&&& \\
1.0 &&&& \\
1.25 &&&& \\
1.5 &&&& \\
2.0 &&&& \\
2.5 &&&& \\
5.0 &&&& \\
\hline
\end{tabular}
\label{tab:results-cost}
\caption{Modulating the cost function to reduce false positives; SVM with s2v representation}
\end{table*}


\section{Results}

Our evaluation is using precision and recall.
This is not a plain multi-way classification task; spurious cues and missed cues are both possible, as are multiple cues per text passage.

How to present these?

Results are given in Table~\ref{tab:results-main}.

The effect of tuning the cost parameter is shown in Table~\ref{tab:results-cost}.

error analysis

per-class accuracy (coarse, fine)


\section{Related Work}

There is extensive literature on data-intensive approaches to document classification~\cite{sebastiani2002machine}.
Given that the representation chosen is dervied from neural net approaches, it may improve things to use a neural net for the classification itself.
We already know that neural nets are good at binary doc classification~\cite{derczynski2006machine}.
Prior work has also examined document classification using SVM~\cite{isa2008text}.
However, little work has address the case of streaming classification of small documents, being primarily focused on batch indexing and retrieval tasks.


\section{Conclusion}


\paragraph*{Acknowledgments}
This research has received partial support from the European Union’s Seventh Framework Programme for research, technological development and demonstration under grant agreement No. 611233, \textsc{Pheme}, and also from The Danish Council for Strategic Research under grant agreement No. 1311-00001B, \textsc{CIBIS}.

\balance

\bibliographystyle{lrec2006}
\bibliography{rpg-scene}

\end{document}